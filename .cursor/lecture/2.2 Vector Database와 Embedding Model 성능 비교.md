# **Vector Database**

---

# **사용자가 원하는 정보**

1. 사용자의 **질문**과 관련있는 데이터
    1. 관련이 있다는 것을 어떻게 판단할까?
    2. 관련성 파악을 위해 vector를 활용함
        1. vector: row 하나짜리 행렬
        2. 단어 또는 문장의 유사도를 파악해서 관련성을 측정함
2. Vector를 생성하는 방법
    1. OpenAI의 Embedding 모델을 활용해서 vector를 생성함
    2. 문장에서 비슷한 단어가 자주 붙어있는 것을 학습
        1. 왕은 왕자의 아버지다
        2. 여왕은 왕자의 어머니다
    3. "왕자의" 라는 단어 앞에 등장하는 "왕"과 "여왕"은 유사할 가능성이 높다
    4. [Embedding Projector](https://projector.tensorflow.org/)
        
        ![image.png](https://cdn.inflearn.com/public/files/posts/8499aec6-7e49-4403-862c-37ce8d6947c0/f5b0c6b0-c335-4906-b67a-1d7738f60911.png)
        

---

# **즉, 벡터DB란**

1. Embedding 모델을 활용해 생성된 vector를 저장하는 DB
    1. 단순히 vector만 저장하면 안되고 **metadata**도 같이 저장
        1. 이 항목이 상당히 중요한데, 뒤에 실습하면서 배울 예정
        2. **문서의 이름, 페이지 번호** 등등을 같이 저장
            1. 이게 나중에 답변의 출처과 되면서, 솔루션의 신뢰도가 증가함
2. Vector를 대상으로 유사도 검색 실시
    1. 사용자의 질문과 가장 비슷한 문서를 가져오는 것 - **Retrieval**
        1. 소득세법을 RAG의 knowleddge base로 활용할 예정
        2. 문서 전체를 활용하면 속도도 느리고, 토큰수 초과로 답변 생성이 안될수도 있음
        3. 그렇기 때문에 문서를 chunking, 나눠서 저장해야 함
            1. LangChain 공식문서도 4만 2천자마다 쪼개어 사용하는 것을 권장하고 있음.
    2. 가져온 문서를 prompt를 통해 LLM에 제공 - **Augmented**
    3. LLM은 prompt를 활용해서 답변 생성 - **Generation**

---

# **실습**

openAI Embedding 모델

- large 모델: 한국어에 적합
- small 모델: 영어에 적합

cosine_similarity

- cosine 으로 각도 재서 유사도(0~1) 반환
- king과 queen 유사도 비교: 0.55
- king과 slave 유사도 비교: 0.29

upstage Embedding 모델

- 우리나라의 경우 upstage embedding 모델이 성능이 좋음
- openAI Embedding 모델:
    - 왕과 king 유사도: 0.54
- upstage Embedding 모델:
    - 왕과 king 유사도: 0.85